{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "0. KarcıFANN FER2013 veriseti ile...  Ilk olarak dataseti indirelim\n",
   "id": "9fa9581ee5d9ae8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "9d305c977b2338c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1) Imports + yardımcı fonksiyonlar",
   "id": "1e2bd4eaae3ba918"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T13:53:31.644333Z",
     "start_time": "2025-12-17T13:53:27.260918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, cv2, numpy as np, mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def onehot(y_idx, num_classes):\n",
    "    return np.eye(num_classes, dtype=np.float32)[y_idx]\n",
    "\n",
    "def l2(x):\n",
    "    return float(np.sqrt(np.sum(x * x)))\n",
    "\n",
    "def majority_vote(items, k=15):\n",
    "    if len(items) == 0:\n",
    "        return None\n",
    "    last = items[-k:]\n",
    "    vals, cnts = np.unique(last, return_counts=True)\n",
    "    return vals[np.argmax(cnts)]\n",
    "\n",
    "def compute_class_weights(y, num_classes):\n",
    "    # inverse frequency (normalize mean=1)\n",
    "    counts = np.bincount(y, minlength=num_classes).astype(np.float32)\n",
    "    w = 1.0 / np.maximum(counts, 1.0)\n",
    "    w = w * (num_classes / np.sum(w))\n",
    "    return w.astype(np.float32)\n",
    "\n",
    "def weighted_cross_entropy(y_pred, y_true_oh, class_w=None, eps=1e-9):\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    logp = np.log(y_pred)\n",
    "    per = -np.sum(y_true_oh * logp, axis=1)  # (batch,)\n",
    "    if class_w is not None:\n",
    "        sw = np.sum(y_true_oh * class_w.reshape(1, -1), axis=1)  # sample weight\n",
    "        per = per * sw\n",
    "    return float(np.mean(per))\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_face_detection = mp.solutions.face_detection\n"
   ],
   "id": "dbe4901afdda1504",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:22:39.622238Z",
     "start_time": "2025-12-17T06:22:39.612283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / (np.sum(e, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "def onehot(y_idx, num_classes):\n",
    "    return np.eye(num_classes, dtype=np.float32)[y_idx]\n",
    "\n",
    "def l2(x):\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    return float(np.sqrt(np.sum(x * x) + 1e-12))\n",
    "\n",
    "def majority_vote(items, k=15):\n",
    "    if len(items) == 0:\n",
    "        return None\n",
    "    last = items[-k:]\n",
    "    vals, cnts = np.unique(last, return_counts=True)\n",
    "    return vals[np.argmax(cnts)]\n"
   ],
   "id": "1409a1d58a4401be",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2) MediaPipe FaceMesh (face_only) stabil pipeline",
   "id": "cb2bde500fdadc9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:22:43.313305Z",
     "start_time": "2025-12-17T06:22:43.272385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "FACE_LM_TARGET = 468  # hedef 468 landmark (refine_landmarks=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocess (FER küçük/kontrast düşük)\n",
    "# -----------------------------\n",
    "def preprocess_for_facemesh(bgr, resize_to=384, use_clahe=True, use_sharpen=True):\n",
    "    img = bgr\n",
    "    if resize_to is not None:\n",
    "        img = cv2.resize(img, (resize_to, resize_to), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    if use_clahe:\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        l2 = clahe.apply(l)\n",
    "        lab = cv2.merge([l2, a, b])\n",
    "        img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    if use_sharpen:\n",
    "        blur = cv2.GaussianBlur(img, (0, 0), 1.2)\n",
    "        img = cv2.addWeighted(img, 1.5, blur, -0.5, 0)\n",
    "\n",
    "    return img\n",
    "\n",
    "def detect_face_bbox(face_det, rgb, w, h, margin=0.25):\n",
    "    det = face_det.process(rgb)\n",
    "    if not det.detections:\n",
    "        return None, 0.0\n",
    "    best = max(det.detections, key=lambda d: d.score[0] if d.score else 0.0)\n",
    "    score = float(best.score[0]) if best.score else 0.0\n",
    "    bbox = best.location_data.relative_bounding_box\n",
    "\n",
    "    x1 = int(bbox.xmin * w); y1 = int(bbox.ymin * h)\n",
    "    bw = int(bbox.width * w); bh = int(bbox.height * h)\n",
    "    x2 = x1 + bw; y2 = y1 + bh\n",
    "\n",
    "    mx = int(bw * margin); my = int(bh * margin)\n",
    "    x1 = max(0, x1 - mx); y1 = max(0, y1 - my)\n",
    "    x2 = min(w - 1, x2 + mx); y2 = min(h - 1, y2 + my)\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None, 0.0\n",
    "    return (x1, y1, x2, y2), score\n",
    "\n",
    "def _unique_indices_from_connections(conns):\n",
    "    s = set()\n",
    "    for a, b in conns:\n",
    "        s.add(a); s.add(b)\n",
    "    return np.array(sorted(list(s)), dtype=np.int32)\n",
    "\n",
    "LEFT_EYE_IDX  = _unique_indices_from_connections(mp_face_mesh.FACEMESH_LEFT_EYE)\n",
    "RIGHT_EYE_IDX = _unique_indices_from_connections(mp_face_mesh.FACEMESH_RIGHT_EYE)\n",
    "\n",
    "def align_landmarks_xyzn(pts_xyz, score, eps=1e-6):\n",
    "    \"\"\"\n",
    "    pts_xyz: (468,3) in pixel-like coordinates (x,y) and z arbitrary scale\n",
    "    returns: (468,4) -> aligned x,y,z plus v=score\n",
    "    \"\"\"\n",
    "    L = pts_xyz[LEFT_EYE_IDX, :2].mean(axis=0)\n",
    "    R = pts_xyz[RIGHT_EYE_IDX, :2].mean(axis=0)\n",
    "\n",
    "    dx, dy = (R - L).astype(np.float32)\n",
    "    dist = float(np.sqrt(dx*dx + dy*dy) + eps)\n",
    "    ang = float(np.arctan2(dy, dx))\n",
    "\n",
    "    c, s = np.cos(-ang), np.sin(-ang)\n",
    "    rot = np.array([[c, -s],[s, c]], dtype=np.float32)\n",
    "\n",
    "    mid = (L + R) / 2.0\n",
    "    xy = pts_xyz[:, :2] - mid.reshape(1, 2)\n",
    "    xy = (xy @ rot.T) / dist\n",
    "\n",
    "    z = pts_xyz[:, 2] / dist  # scale z similarly\n",
    "    v = np.full((pts_xyz.shape[0], 1), float(score), dtype=np.float32)\n",
    "\n",
    "    out = np.concatenate([xy.astype(np.float32), z.reshape(-1,1).astype(np.float32), v], axis=1)\n",
    "    return out\n",
    "\n",
    "def facemesh_features_from_bgr(face_det, face_mesh, bgr, resize_to=384):\n",
    "    \"\"\"\n",
    "    Output: 468*4 = 1872 feature (aligned + score)\n",
    "    Tries multiple ways to reduce skip:\n",
    "      1) preprocessed resize\n",
    "      2) face crop with FaceDetection\n",
    "      3) full frame fallback\n",
    "      4) multiscale fallback\n",
    "    \"\"\"\n",
    "    # multi-scale attempts (büyük -> küçük)\n",
    "    for rt in [resize_to, 512, 384, 256]:\n",
    "        if rt is None:\n",
    "            continue\n",
    "\n",
    "        img = preprocess_for_facemesh(bgr, resize_to=rt, use_clahe=True, use_sharpen=True)\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = rgb.shape[:2]\n",
    "\n",
    "        bbox, score = detect_face_bbox(face_det, rgb, w, h, margin=0.30)\n",
    "\n",
    "        # candidate 1: crop\n",
    "        if bbox is not None:\n",
    "            x1,y1,x2,y2 = bbox\n",
    "            crop = rgb[y1:y2, x1:x2]\n",
    "            if crop.size > 0:\n",
    "                res = face_mesh.process(crop)\n",
    "                if res.multi_face_landmarks:\n",
    "                    lm = res.multi_face_landmarks[0].landmark\n",
    "                    pts = np.zeros((468,3), dtype=np.float32)\n",
    "                    # landmark x,y are normalized to crop\n",
    "                    cw = max(1, crop.shape[1]); ch = max(1, crop.shape[0])\n",
    "                    for i, p in enumerate(lm[:468]):  # safeguard\n",
    "                        pts[i,0] = (p.x * cw) + x1\n",
    "                        pts[i,1] = (p.y * ch) + y1\n",
    "                        pts[i,2] = p.z  # z already relative\n",
    "                    aligned = align_landmarks_xyzn(pts, score)\n",
    "                    return aligned.reshape(-1)\n",
    "\n",
    "        # candidate 2: full frame fallback\n",
    "        res2 = face_mesh.process(rgb)\n",
    "        if res2.multi_face_landmarks:\n",
    "            lm = res2.multi_face_landmarks[0].landmark\n",
    "            pts = np.zeros((468,3), dtype=np.float32)\n",
    "            for i, p in enumerate(lm[:468]):\n",
    "                pts[i,0] = p.x * w\n",
    "                pts[i,1] = p.y * h\n",
    "                pts[i,2] = p.z\n",
    "            aligned = align_landmarks_xyzn(pts, score)\n",
    "            return aligned.reshape(-1)\n",
    "\n",
    "    return None\n",
    "\n",
    "def expected_feature_dim(mode=\"face_only\"):\n",
    "    if mode != \"face_only\":\n",
    "        raise ValueError(\"Bu pipeline sadece face_only için optimize edildi.\")\n",
    "    return 468 * 4\n",
    "\n",
    "def extract_fer2013_folder(root, out=\"fer_mp_align.npz\", resize_to=384, max_per_class=None):\n",
    "    train_dir = os.path.join(root, \"train\")\n",
    "    test_dir  = os.path.join(root, \"test\")\n",
    "    if not os.path.isdir(train_dir) or not os.path.isdir(test_dir):\n",
    "        raise RuntimeError(f\"train/test klasörleri bulunamadı: {train_dir} , {test_dir}\")\n",
    "\n",
    "    classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "    if not classes:\n",
    "        raise RuntimeError(\"train içinde class klasörleri bulunamadı.\")\n",
    "\n",
    "    dim = expected_feature_dim(\"face_only\")\n",
    "    print(\"Classes:\", classes)\n",
    "\n",
    "    def list_images(folder):\n",
    "        imgs = [f for f in os.listdir(folder) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        imgs.sort()\n",
    "        if max_per_class is not None:\n",
    "            imgs = imgs[:max_per_class]\n",
    "        return imgs\n",
    "\n",
    "    Xtr, ytr, Xte, yte = [], [], [], []\n",
    "    skipped = {\"train\": 0, \"test\": 0}\n",
    "    used = {\"train\": 0, \"test\": 0}\n",
    "\n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.15) as face_det, \\\n",
    "         mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1,\n",
    "                               refine_landmarks=False,\n",
    "                               min_detection_confidence=0.15,\n",
    "                               min_tracking_confidence=0.15) as face_mesh:\n",
    "\n",
    "        def process_split(split_name, split_path, X_list, y_list):\n",
    "            for ci, cls in enumerate(classes):\n",
    "                folder = os.path.join(split_path, cls)\n",
    "                imgs = list_images(folder)\n",
    "                print(f\"[{split_name}] {cls}: {len(imgs)} görüntü\")\n",
    "\n",
    "                for fn in imgs:\n",
    "                    p = os.path.join(folder, fn)\n",
    "                    img = cv2.imread(p)\n",
    "                    if img is None:\n",
    "                        skipped[split_name] += 1\n",
    "                        continue\n",
    "\n",
    "                    feat = facemesh_features_from_bgr(face_det, face_mesh, img, resize_to=resize_to)\n",
    "                    if feat is None or feat.shape[0] != dim:\n",
    "                        skipped[split_name] += 1\n",
    "                        continue\n",
    "\n",
    "                    X_list.append(feat)\n",
    "                    y_list.append(ci)\n",
    "                    used[split_name] += 1\n",
    "\n",
    "        process_split(\"train\", train_dir, Xtr, ytr)\n",
    "        process_split(\"test\",  test_dir,  Xte, yte)\n",
    "\n",
    "    if len(Xtr) == 0 or len(Xte) == 0:\n",
    "        raise RuntimeError(\"Hiç veri çıkmadı. resize_to=512 dene veya dataset yolunu kontrol et.\")\n",
    "\n",
    "    X_train = np.stack(Xtr, axis=0).astype(np.float32)\n",
    "    y_train = np.array(ytr, dtype=np.int64)\n",
    "    X_test  = np.stack(Xte, axis=0).astype(np.float32)\n",
    "    y_test  = np.array(yte, dtype=np.int64)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        out,\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test,   y_test=y_test,\n",
    "        classes=np.array(classes),\n",
    "        mode=\"face_only\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nSaved:\", out)\n",
    "    print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "    print(\"Used:\", used, \"Skipped:\", skipped)\n",
    "\n",
    "def detect_face_bbox(face_det, rgb, w, h, margin=0.25):\n",
    "    det = face_det.process(rgb)\n",
    "    if not det.detections:\n",
    "        return None, 0.0\n",
    "    best = max(det.detections, key=lambda d: d.score[0] if d.score else 0.0)\n",
    "    score = float(best.score[0]) if best.score else 0.0\n",
    "    bbox = best.location_data.relative_bounding_box\n",
    "\n",
    "    x1 = int(bbox.xmin * w)\n",
    "    y1 = int(bbox.ymin * h)\n",
    "    bw = int(bbox.width * w)\n",
    "    bh = int(bbox.height * h)\n",
    "    x2 = x1 + bw\n",
    "    y2 = y1 + bh\n",
    "\n",
    "    mx = int(bw * margin)\n",
    "    my = int(bh * margin)\n",
    "    x1 = max(0, x1 - mx)\n",
    "    y1 = max(0, y1 - my)\n",
    "    x2 = min(w - 1, x2 + mx)\n",
    "    y2 = min(h - 1, y2 + my)\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None, 0.0\n",
    "    return (x1, y1, x2, y2), score\n",
    "\n",
    "def facemesh_features_from_bgr(face_det, face_mesh, bgr, resize_to=384):\n",
    "    \"\"\"\n",
    "    468*(x,y,z,score)=1872 boyut.\n",
    "    - bbox crop + full frame fallback\n",
    "    - 478 gelirse otomatik 468'e kırpma/padding (IndexError biter)\n",
    "    \"\"\"\n",
    "    img = preprocess_for_facemesh(bgr, resize_to=resize_to, use_clahe=True, use_sharpen=True)\n",
    "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = rgb.shape[:2]\n",
    "\n",
    "    bbox, score = detect_face_bbox(face_det, rgb, w, h, margin=0.25)\n",
    "\n",
    "    candidates = []\n",
    "    if bbox is not None:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        crop = rgb[y1:y2, x1:x2]\n",
    "        if crop.size > 0:\n",
    "            candidates.append((face_mesh.process(crop), score))\n",
    "\n",
    "    candidates.append((face_mesh.process(rgb), score))\n",
    "\n",
    "    for res, sc in candidates:\n",
    "        if res.multi_face_landmarks:\n",
    "            lm = res.multi_face_landmarks[0].landmark\n",
    "            n = len(lm)\n",
    "\n",
    "            feat = np.zeros((FACE_LM_TARGET, 4), dtype=np.float32)\n",
    "            use_n = min(FACE_LM_TARGET, n)\n",
    "            for i in range(use_n):\n",
    "                p = lm[i]\n",
    "                feat[i, 0] = float(p.x)\n",
    "                feat[i, 1] = float(p.y)\n",
    "                feat[i, 2] = float(p.z)\n",
    "                feat[i, 3] = float(sc)\n",
    "\n",
    "            return feat.reshape(-1)\n",
    "\n",
    "    return None\n",
    "\n",
    "def expected_feature_dim(mode):\n",
    "    dim = 468 * 4\n",
    "    if mode in (\"face_pose\", \"face_pose_hands\"):\n",
    "        dim += 33 * 4\n",
    "    if mode == \"face_pose_hands\":\n",
    "        dim += 21 * 4 + 21 * 4\n",
    "    return dim\n"
   ],
   "id": "af0745c690ef4bbf",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3) Holistic (opsiyonel modlar için)",
   "id": "1e09dd5c6de308dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:23:02.295734Z",
     "start_time": "2025-12-17T06:23:02.287350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _lm4(lm):\n",
    "    v = getattr(lm, \"visibility\", None)\n",
    "    if v is None:\n",
    "        v = getattr(lm, \"presence\", 0.0)\n",
    "    return [float(lm.x), float(lm.y), float(lm.z), float(v)]\n",
    "\n",
    "def extract_features_from_results(results, mode=\"face_only\"):\n",
    "    parts = []\n",
    "    want_pose = mode in (\"face_pose\", \"face_pose_hands\")\n",
    "    want_hands = mode == \"face_pose_hands\"\n",
    "\n",
    "    if results.face_landmarks is not None:\n",
    "        face = results.face_landmarks.landmark\n",
    "        parts.append(np.array([_lm4(lm) for lm in face], dtype=np.float32).flatten())\n",
    "    else:\n",
    "        parts.append(np.zeros((468 * 4,), dtype=np.float32))\n",
    "\n",
    "    if want_pose:\n",
    "        if results.pose_landmarks is not None:\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            parts.append(np.array([_lm4(lm) for lm in pose], dtype=np.float32).flatten())\n",
    "        else:\n",
    "            parts.append(np.zeros((33 * 4,), dtype=np.float32))\n",
    "\n",
    "    if want_hands:\n",
    "        if results.left_hand_landmarks is not None:\n",
    "            lh = results.left_hand_landmarks.landmark\n",
    "            parts.append(np.array([_lm4(lm) for lm in lh], dtype=np.float32).flatten())\n",
    "        else:\n",
    "            parts.append(np.zeros((21 * 4,), dtype=np.float32))\n",
    "\n",
    "        if results.right_hand_landmarks is not None:\n",
    "            rh = results.right_hand_landmarks.landmark\n",
    "            parts.append(np.array([_lm4(lm) for lm in rh], dtype=np.float32).flatten())\n",
    "        else:\n",
    "            parts.append(np.zeros((21 * 4,), dtype=np.float32))\n",
    "\n",
    "    return np.concatenate(parts, axis=0).astype(np.float32)\n"
   ],
   "id": "e08c49f01eddcb66",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4) Dataset extract (fer2013 klasöründen npz)",
   "id": "1a00c462b1d0054"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:23:06.234292Z",
     "start_time": "2025-12-17T06:23:06.202714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_fer2013_folder(root, mode=\"face_only\", out=\"fer_mp.npz\", resize_to=384, max_per_class=None):\n",
    "    train_dir = os.path.join(root, \"train\")\n",
    "    test_dir  = os.path.join(root, \"test\")\n",
    "\n",
    "    if not os.path.isdir(train_dir) or not os.path.isdir(test_dir):\n",
    "        raise RuntimeError(f\"train/test klasörleri bulunamadı: {train_dir} , {test_dir}\")\n",
    "\n",
    "    classes = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "    if not classes:\n",
    "        raise RuntimeError(\"train içinde class klasörleri bulunamadı.\")\n",
    "\n",
    "    print(\"Classes:\", classes)\n",
    "    dim = expected_feature_dim(mode)\n",
    "\n",
    "    Xtr, ytr, Xte, yte = [], [], [], []\n",
    "    skipped = {\"train\": 0, \"test\": 0}\n",
    "    used = {\"train\": 0, \"test\": 0}\n",
    "\n",
    "    def list_images(folder):\n",
    "        imgs = [f for f in os.listdir(folder) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        imgs.sort()\n",
    "        if max_per_class is not None:\n",
    "            imgs = imgs[:max_per_class]\n",
    "        return imgs\n",
    "\n",
    "    if mode == \"face_only\":\n",
    "        with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.2) as face_det, \\\n",
    "             mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1,\n",
    "                                   refine_landmarks=False,\n",
    "                                   min_detection_confidence=0.2,\n",
    "                                   min_tracking_confidence=0.2) as face_mesh:\n",
    "\n",
    "            def process_split(split_name, split_path, X_list, y_list):\n",
    "                for ci, cls in enumerate(classes):\n",
    "                    folder = os.path.join(split_path, cls)\n",
    "                    imgs = list_images(folder)\n",
    "                    print(f\"[{split_name}] {cls}: {len(imgs)} görüntü\")\n",
    "\n",
    "                    for fn in imgs:\n",
    "                        p = os.path.join(folder, fn)\n",
    "                        img = cv2.imread(p)\n",
    "                        if img is None:\n",
    "                            skipped[split_name] += 1\n",
    "                            continue\n",
    "\n",
    "                        feat = facemesh_features_from_bgr(face_det, face_mesh, img, resize_to=resize_to)\n",
    "                        if feat is None or feat.shape[0] != dim:\n",
    "                            skipped[split_name] += 1\n",
    "                            continue\n",
    "\n",
    "                        X_list.append(feat)\n",
    "                        y_list.append(ci)\n",
    "                        used[split_name] += 1\n",
    "\n",
    "            process_split(\"train\", train_dir, Xtr, ytr)\n",
    "            process_split(\"test\",  test_dir,  Xte, yte)\n",
    "\n",
    "    else:\n",
    "        with mp_holistic.Holistic(\n",
    "            static_image_mode=True,\n",
    "            model_complexity=1,\n",
    "            refine_face_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        ) as holistic:\n",
    "\n",
    "            def process_split(split_name, split_path, X_list, y_list):\n",
    "                for ci, cls in enumerate(classes):\n",
    "                    folder = os.path.join(split_path, cls)\n",
    "                    imgs = list_images(folder)\n",
    "                    print(f\"[{split_name}] {cls}: {len(imgs)} görüntü\")\n",
    "\n",
    "                    for fn in imgs:\n",
    "                        p = os.path.join(folder, fn)\n",
    "                        img = cv2.imread(p)\n",
    "                        if img is None:\n",
    "                            skipped[split_name] += 1\n",
    "                            continue\n",
    "\n",
    "                        if resize_to is not None:\n",
    "                            img = cv2.resize(img, (resize_to, resize_to), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        rgb.flags.writeable = False\n",
    "                        res = holistic.process(rgb)\n",
    "\n",
    "                        feat = extract_features_from_results(res, mode=mode)\n",
    "                        if feat.shape[0] != dim:\n",
    "                            skipped[split_name] += 1\n",
    "                            continue\n",
    "\n",
    "                        X_list.append(feat)\n",
    "                        y_list.append(ci)\n",
    "                        used[split_name] += 1\n",
    "\n",
    "            process_split(\"train\", train_dir, Xtr, ytr)\n",
    "            process_split(\"test\",  test_dir,  Xte, yte)\n",
    "\n",
    "    if len(Xtr) == 0 or len(Xte) == 0:\n",
    "        raise RuntimeError(\"Hiç veri çıkmadı. resize_to=512 dene veya dataset yolunu kontrol et.\")\n",
    "\n",
    "    X_train = np.stack(Xtr, axis=0).astype(np.float32)\n",
    "    y_train = np.array(ytr, dtype=np.int64)\n",
    "    X_test  = np.stack(Xte, axis=0).astype(np.float32)\n",
    "    y_test  = np.array(yte, dtype=np.int64)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        out,\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test,   y_test=y_test,\n",
    "        classes=np.array(classes),\n",
    "        mode=mode\n",
    "    )\n",
    "\n",
    "    print(\"\\nSaved:\", out)\n",
    "    print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "    print(\"Used:\", used, \"Skipped:\", skipped)\n"
   ],
   "id": "6b6a68d98d52245",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5) Balanced batch sampler",
   "id": "753f3c48b57ff166"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:23:18.792954Z",
     "start_time": "2025-12-17T06:23:18.786456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def balanced_batch_indices(y, batch_size, rng):\n",
    "    classes = np.unique(y)\n",
    "    per = max(1, batch_size // len(classes))\n",
    "    idxs = []\n",
    "    for c in classes:\n",
    "        pool = np.where(y == c)[0]\n",
    "        pick = rng.choice(pool, size=per, replace=True)\n",
    "        idxs.append(pick)\n",
    "    idx = np.concatenate(idxs)\n",
    "    if idx.size < batch_size:\n",
    "        extra = rng.choice(np.arange(len(y)), size=(batch_size - idx.size), replace=True)\n",
    "        idx = np.concatenate([idx, extra])\n",
    "    rng.shuffle(idx)\n",
    "    return idx[:batch_size]\n"
   ],
   "id": "25c39f6b11fb2471",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6) MLP Base + SGD + Adam (Adam fixli)",
   "id": "eee6428c70ac3037"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:23:21.334915Z",
     "start_time": "2025-12-17T06:23:21.311542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Base MLP (2-layer)\n",
    "# -----------------------------\n",
    "class BaseMLP:\n",
    "    def __init__(self, in_dim, hidden, out_dim, seed=42, weight_decay=1e-4):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.W1 = rng.normal(0, 0.2, size=(in_dim, hidden)).astype(np.float32)\n",
    "        self.b1 = np.zeros((1, hidden), dtype=np.float32)\n",
    "        self.W2 = rng.normal(0, 0.2, size=(hidden, out_dim)).astype(np.float32)\n",
    "        self.b2 = np.zeros((1, out_dim), dtype=np.float32)\n",
    "\n",
    "        self.weight_decay = float(weight_decay)\n",
    "\n",
    "        self.loss_hist = []\n",
    "        self.acc_hist = []\n",
    "        self.wdelta_hist = []\n",
    "        self.step_hist = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        # ReLU tanh’a göre genelde daha iyi\n",
    "        self.a1 = np.maximum(self.z1, 0.0)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def predict(self, X):\n",
    "        p = self.forward(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "\n",
    "    def _grads(self, Xb, Yb, class_w=None):\n",
    "        m = Xb.shape[0]\n",
    "        y_pred = self.forward(Xb)\n",
    "        loss = weighted_cross_entropy(y_pred, Yb, class_w=class_w)\n",
    "\n",
    "        # weighted gradient\n",
    "        if class_w is not None:\n",
    "            sw = np.sum(Yb * class_w.reshape(1, -1), axis=1).reshape(-1, 1)\n",
    "        else:\n",
    "            sw = 1.0\n",
    "\n",
    "        dz2 = (y_pred - Yb) * (sw / m)\n",
    "        dW2 = self.a1.T @ dz2 + self.weight_decay * self.W2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * (self.z1 > 0).astype(np.float32)\n",
    "        dW1 = Xb.T @ dz1 + self.weight_decay * self.W1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "        return float(loss), (dW1, db1, dW2, db2)\n",
    "\n",
    "    def _wvec(self):\n",
    "        return np.concatenate([self.W1.ravel(), self.b1.ravel(), self.W2.ravel(), self.b2.ravel()]).astype(np.float32)\n",
    "\n",
    "def clip_by_norm(G, max_norm=5.0, eps=1e-9):\n",
    "    n = float(np.sqrt(np.sum(G*G)) + eps)\n",
    "    if n > max_norm:\n",
    "        return G * (max_norm / n)\n",
    "    return G\n",
    "\n",
    "class MLP_SGD(BaseMLP):\n",
    "    def __init__(self, in_dim, hidden, out_dim, lr=0.03, seed=42, weight_decay=1e-4):\n",
    "        super().__init__(in_dim, hidden, out_dim, seed=seed, weight_decay=weight_decay)\n",
    "        self.lr = float(lr)\n",
    "\n",
    "    def train_epoch(self, X, Y, class_w=None, batch_size=128, seed=0):\n",
    "        w0 = self._wvec()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.permutation(X.shape[0])\n",
    "\n",
    "        total_loss = 0.0\n",
    "        step_sum = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        for s in range(0, len(idx), batch_size):\n",
    "            b = idx[s:s+batch_size]\n",
    "            Xb, Yb = X[b], Y[b]\n",
    "            loss, (dW1, db1, dW2, db2) = self._grads(Xb, Yb, class_w=class_w)\n",
    "\n",
    "            dW1 = clip_by_norm(dW1, 5.0); dW2 = clip_by_norm(dW2, 5.0)\n",
    "\n",
    "            uW1 = self.lr * dW1; ub1 = self.lr * db1\n",
    "            uW2 = self.lr * dW2; ub2 = self.lr * db2\n",
    "\n",
    "            self.W1 -= uW1; self.b1 -= ub1\n",
    "            self.W2 -= uW2; self.b2 -= ub2\n",
    "\n",
    "            total_loss += loss * Xb.shape[0]\n",
    "            step_sum += (l2(uW1) + l2(ub1) + l2(uW2) + l2(ub2))\n",
    "            steps += 1\n",
    "\n",
    "        w1 = self._wvec()\n",
    "        self.wdelta_hist.append(l2(w1 - w0))\n",
    "        self.step_hist.append(step_sum / max(1, steps))\n",
    "        return total_loss / X.shape[0]\n",
    "\n",
    "class MLP_Adam(BaseMLP):\n",
    "    def __init__(self, in_dim, hidden, out_dim, lr=0.0015, beta1=0.9, beta2=0.999, eps=1e-8, seed=42, weight_decay=1e-4):\n",
    "        super().__init__(in_dim, hidden, out_dim, seed=seed, weight_decay=weight_decay)\n",
    "        self.lr = float(lr); self.beta1=float(beta1); self.beta2=float(beta2); self.eps=float(eps)\n",
    "        self.t = 0\n",
    "        self.mW1 = np.zeros_like(self.W1); self.vW1 = np.zeros_like(self.W1)\n",
    "        self.mb1 = np.zeros_like(self.b1); self.vb1 = np.zeros_like(self.b1)\n",
    "        self.mW2 = np.zeros_like(self.W2); self.vW2 = np.zeros_like(self.W2)\n",
    "        self.mb2 = np.zeros_like(self.b2); self.vb2 = np.zeros_like(self.b2)\n",
    "\n",
    "    def _upd(self, P, G, M, V):\n",
    "        self.t += 1\n",
    "        M[:] = self.beta1 * M + (1 - self.beta1) * G\n",
    "        V[:] = self.beta2 * V + (1 - self.beta2) * (G * G)\n",
    "        Mh = M / (1 - self.beta1 ** self.t)\n",
    "        Vh = V / (1 - self.beta2 ** self.t)\n",
    "        U = self.lr * Mh / (np.sqrt(Vh) + self.eps)\n",
    "        P[:] = P - U\n",
    "        return U\n",
    "\n",
    "    def train_epoch(self, X, Y, class_w=None, batch_size=128, seed=0):\n",
    "        w0 = self._wvec()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.permutation(X.shape[0])\n",
    "\n",
    "        total_loss = 0.0\n",
    "        step_sum = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        for s in range(0, len(idx), batch_size):\n",
    "            b = idx[s:s+batch_size]\n",
    "            Xb, Yb = X[b], Y[b]\n",
    "            loss, (dW1, db1, dW2, db2) = self._grads(Xb, Yb, class_w=class_w)\n",
    "\n",
    "            dW1 = clip_by_norm(dW1, 5.0); dW2 = clip_by_norm(dW2, 5.0)\n",
    "\n",
    "            uW1 = self._upd(self.W1, dW1, self.mW1, self.vW1)\n",
    "            ub1 = self._upd(self.b1, db1, self.mb1, self.vb1)\n",
    "            uW2 = self._upd(self.W2, dW2, self.mW2, self.vW2)\n",
    "            ub2 = self._upd(self.b2, db2, self.mb2, self.vb2)\n",
    "\n",
    "            total_loss += loss * Xb.shape[0]\n",
    "            step_sum += (l2(uW1) + l2(ub1) + l2(uW2) + l2(ub2))\n",
    "            steps += 1\n",
    "\n",
    "        w1 = self._wvec()\n",
    "        self.wdelta_hist.append(l2(w1 - w0))\n",
    "        self.step_hist.append(step_sum / max(1, steps))\n",
    "        return total_loss / X.shape[0]\n"
   ],
   "id": "da98dfa7bc7b7ef5",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7) KarcıFANN + Momentum-Karcı (ABS YOK, önceki pozitif ağırlık ref)",
   "id": "1934c0ea3028c0f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:23:27.026744Z",
     "start_time": "2025-12-17T06:23:27.000719Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 36,
   "source": [
    "class PositiveRef:\n",
    "    def __init__(self, W, w_floor=1e-2):\n",
    "        self.w_floor = float(w_floor)\n",
    "        self.ref = np.maximum(np.abs(W).astype(np.float32), self.w_floor)\n",
    "\n",
    "    def update(self, W):\n",
    "        W = W.astype(np.float32)\n",
    "        pos = W > 0\n",
    "        self.ref[pos] = W[pos]\n",
    "        self.ref = np.maximum(self.ref, self.w_floor)\n",
    "        return self.ref\n",
    "\n",
    "# -----------------------------\n",
    "# Karcı scale (math same, ref fixed)\n",
    "# -----------------------------\n",
    "def karci_scale(grad, W_ref_pos, loss_scalar, alpha, eps=1e-3, clip=0.20, r_clip=(1e-3, 1e3)):\n",
    "    \"\"\"\n",
    "    SAME math:\n",
    "      num = (loss+eps)^(alpha-1)\n",
    "      den = (W_ref+eps)^(alpha-1)\n",
    "      u = (num/den) * grad\n",
    "\n",
    "    Stability:\n",
    "      - W_ref_pos forced positive (prev-positive logic outside)\n",
    "      - ratio clipping to avoid explosions when alpha<1 or alpha>>1\n",
    "    \"\"\"\n",
    "    p = float(alpha - 1.0)\n",
    "    loss = float(loss_scalar)\n",
    "    num = (loss + eps)\n",
    "    den = (W_ref_pos + eps)\n",
    "\n",
    "    # ratio^p\n",
    "    ratio = np.clip(num / den, r_clip[0], r_clip[1])\n",
    "    scale = ratio ** p\n",
    "    u = scale * grad\n",
    "\n",
    "    if clip is not None:\n",
    "        u = np.clip(u, -clip, clip)\n",
    "    return u\n",
    "\n",
    "class MLP_KarciFANN(BaseMLP):\n",
    "    def __init__(self, in_dim, hidden, out_dim, alpha=1.2, clip=0.10, eps=1e-3, seed=42, weight_decay=1e-4):\n",
    "        super().__init__(in_dim, hidden, out_dim, seed=seed, weight_decay=weight_decay)\n",
    "        self.alpha = float(alpha)\n",
    "        self.clip = float(clip)\n",
    "        self.eps = float(eps)\n",
    "\n",
    "        # hocanın dediği mantık: negatif olursa önceki pozitif referans\n",
    "        self.W1_ref = np.abs(self.W1).astype(np.float32) + self.eps\n",
    "        self.W2_ref = np.abs(self.W2).astype(np.float32) + self.eps\n",
    "\n",
    "    def _update_refs(self):\n",
    "        self.W1_ref = np.where(self.W1 > 0, self.W1, self.W1_ref)\n",
    "        self.W2_ref = np.where(self.W2 > 0, self.W2, self.W2_ref)\n",
    "\n",
    "    def train_epoch(self, X, Y, class_w=None, batch_size=128, seed=0):\n",
    "        w0 = self._wvec()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.permutation(X.shape[0])\n",
    "\n",
    "        total_loss = 0.0\n",
    "        step_sum = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        for s in range(0, len(idx), batch_size):\n",
    "            b = idx[s:s+batch_size]\n",
    "            Xb, Yb = X[b], Y[b]\n",
    "            loss, (dW1, db1, dW2, db2) = self._grads(Xb, Yb, class_w=class_w)\n",
    "\n",
    "            dW1 = clip_by_norm(dW1, 5.0); dW2 = clip_by_norm(dW2, 5.0)\n",
    "\n",
    "            # W_ref positive guaranteed\n",
    "            W1_pos = np.maximum(self.W1_ref, self.eps).astype(np.float32)\n",
    "            W2_pos = np.maximum(self.W2_ref, self.eps).astype(np.float32)\n",
    "            b1_pos = np.maximum(np.abs(self.b1), self.eps).astype(np.float32)\n",
    "            b2_pos = np.maximum(np.abs(self.b2), self.eps).astype(np.float32)\n",
    "\n",
    "            uW1 = karci_scale(dW1, W1_pos, loss, self.alpha, eps=self.eps, clip=self.clip)\n",
    "            ub1 = karci_scale(db1, b1_pos, loss, self.alpha, eps=self.eps, clip=self.clip)\n",
    "            uW2 = karci_scale(dW2, W2_pos, loss, self.alpha, eps=self.eps, clip=self.clip)\n",
    "            ub2 = karci_scale(db2, b2_pos, loss, self.alpha, eps=self.eps, clip=self.clip)\n",
    "\n",
    "            self.W1 -= uW1; self.b1 -= ub1\n",
    "            self.W2 -= uW2; self.b2 -= ub2\n",
    "\n",
    "            self._update_refs()\n",
    "\n",
    "            total_loss += loss * Xb.shape[0]\n",
    "            step_sum += (l2(uW1) + l2(ub1) + l2(uW2) + l2(ub2))\n",
    "            steps += 1\n",
    "\n",
    "        w1 = self._wvec()\n",
    "        self.wdelta_hist.append(l2(w1 - w0))\n",
    "        self.step_hist.append(step_sum / max(1, steps))\n",
    "        return total_loss / X.shape[0]\n",
    "\n",
    "class MLP_Momentum_Karci(BaseMLP):\n",
    "    def __init__(self, in_dim, hidden, out_dim, alpha=1.25, beta=0.85, clip=0.10, eps=1e-3, seed=42, weight_decay=1e-4):\n",
    "        super().__init__(in_dim, hidden, out_dim, seed=seed, weight_decay=weight_decay)\n",
    "        self.alpha=float(alpha); self.beta=float(beta)\n",
    "        self.clip=float(clip); self.eps=float(eps)\n",
    "\n",
    "        self.vW1 = np.zeros_like(self.W1); self.vb1 = np.zeros_like(self.b1)\n",
    "        self.vW2 = np.zeros_like(self.W2); self.vb2 = np.zeros_like(self.b2)\n",
    "\n",
    "        self.W1_ref = np.abs(self.W1).astype(np.float32) + self.eps\n",
    "        self.W2_ref = np.abs(self.W2).astype(np.float32) + self.eps\n",
    "\n",
    "    def _update_refs(self):\n",
    "        self.W1_ref = np.where(self.W1 > 0, self.W1, self.W1_ref)\n",
    "        self.W2_ref = np.where(self.W2 > 0, self.W2, self.W2_ref)\n",
    "\n",
    "    def train_epoch(self, X, Y, class_w=None, batch_size=128, seed=0):\n",
    "        w0 = self._wvec()\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.permutation(X.shape[0])\n",
    "\n",
    "        total_loss = 0.0\n",
    "        step_sum = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        for s in range(0, len(idx), batch_size):\n",
    "            b = idx[s:s+batch_size]\n",
    "            Xb, Yb = X[b], Y[b]\n",
    "            loss, (dW1, db1, dW2, db2) = self._grads(Xb, Yb, class_w=class_w)\n",
    "\n",
    "            dW1 = clip_by_norm(dW1, 5.0); dW2 = clip_by_norm(dW2, 5.0)\n",
    "\n",
    "            W1_pos = np.maximum(self.W1_ref, self.eps).astype(np.float32)\n",
    "            W2_pos = np.maximum(self.W2_ref, self.eps).astype(np.float32)\n",
    "            b1_pos = np.maximum(np.abs(self.b1), self.eps).astype(np.float32)\n",
    "            b2_pos = np.maximum(np.abs(self.b2), self.eps).astype(np.float32)\n",
    "\n",
    "            gW1 = karci_scale(dW1, W1_pos, loss, self.alpha, eps=self.eps, clip=self.clip)\n",
    "            gb1 = karci_scale(db1, b1_pos, loss, self.alpha, eps=self.eps, clip=self.clip)\n",
    "            gW2 = karci_scale(dW2, W2_pos, loss, self.alpha, eps=self.eps, clip=self.clip)\n",
    "            gb2 = karci_scale(db2, b2_pos, loss, self.alpha, eps=self.eps, clip=self.clip)\n",
    "\n",
    "            self.vW1 = self.beta * self.vW1 + gW1\n",
    "            self.vb1 = self.beta * self.vb1 + gb1\n",
    "            self.vW2 = self.beta * self.vW2 + gW2\n",
    "            self.vb2 = self.beta * self.vb2 + gb2\n",
    "\n",
    "            self.W1 -= self.vW1; self.b1 -= self.vb1\n",
    "            self.W2 -= self.vW2; self.b2 -= self.vb2\n",
    "\n",
    "            self._update_refs()\n",
    "\n",
    "            total_loss += loss * Xb.shape[0]\n",
    "            step_sum += (l2(self.vW1) + l2(self.vb1) + l2(self.vW2) + l2(self.vb2))\n",
    "            steps += 1\n",
    "\n",
    "        w1 = self._wvec()\n",
    "        self.wdelta_hist.append(l2(w1 - w0))\n",
    "        self.step_hist.append(step_sum / max(1, steps))\n",
    "        return total_loss / X.shape[0]\n",
    "\n",
    "\n"
   ],
   "id": "95ee1bffe3a25d7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8) train_compare (final)",
   "id": "d2b6dc6c665de4f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:23:32.430758Z",
     "start_time": "2025-12-17T06:23:32.402684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Train + Compare (early stop + best weights)\n",
    "# -----------------------------\n",
    "def train_compare(data_npz, epochs=120, hidden=512, batch=128, seed=42, save_model=\"fer_model_best.npz\"):\n",
    "    d = np.load(data_npz, allow_pickle=True)\n",
    "    X_train = d[\"X_train\"].astype(np.float32)\n",
    "    y_train = d[\"y_train\"].astype(np.int64)\n",
    "    X_test  = d[\"X_test\"].astype(np.float32)\n",
    "    y_test  = d[\"y_test\"].astype(np.int64)\n",
    "    classes = d[\"classes\"].astype(str)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_test_s  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "    num_classes = len(classes)\n",
    "    Y_train_oh = onehot(y_train, num_classes)\n",
    "\n",
    "    class_w = compute_class_weights(y_train, num_classes)\n",
    "    print(\"Train:\", X_train_s.shape, \"Test:\", X_test_s.shape, \"Classes:\", classes)\n",
    "    print(\"Class weights:\", np.round(class_w, 3))\n",
    "\n",
    "    in_dim = X_train_s.shape[1]\n",
    "\n",
    "    # alpha aralığı: 0.8 - 1.8\n",
    "    # pratikte FaceMesh + bu MLP için iyi başlangıçlar:\n",
    "    models = [\n",
    "        (\"SGD\", MLP_SGD(in_dim, hidden, num_classes, lr=0.03, seed=seed, weight_decay=2e-4)),\n",
    "        (\"Adam\", MLP_Adam(in_dim, hidden, num_classes, lr=0.0015, seed=seed, weight_decay=2e-4)),\n",
    "        (\"KarcıFANN\", MLP_KarciFANN(in_dim, hidden, num_classes, alpha=1.25, clip=0.10, eps=1e-3, seed=seed, weight_decay=2e-4)),\n",
    "        (\"Momentum-Karcı\", MLP_Momentum_Karci(in_dim, hidden, num_classes, alpha=1.30, beta=0.85, clip=0.10, eps=1e-3, seed=seed, weight_decay=2e-4)),\n",
    "    ]\n",
    "\n",
    "    best_global = (-1.0, None, None)  # (acc, name, state)\n",
    "\n",
    "    for name, m in models:\n",
    "        best_acc = -1.0\n",
    "        best_ep = 0\n",
    "        best_state = None\n",
    "\n",
    "        for ep in range(1, epochs + 1):\n",
    "            loss = m.train_epoch(X_train_s, Y_train_oh, class_w=class_w, batch_size=batch, seed=seed + ep)\n",
    "            pred = m.predict(X_test_s)\n",
    "            acc = accuracy_score(y_test, pred)\n",
    "\n",
    "            m.loss_hist.append(loss)\n",
    "            m.acc_hist.append(float(acc))\n",
    "\n",
    "            if acc > best_acc:\n",
    "                best_acc = float(acc)\n",
    "                best_ep = ep\n",
    "                best_state = (m.W1.copy(), m.b1.copy(), m.W2.copy(), m.b2.copy())\n",
    "\n",
    "            if ep % 10 == 0:\n",
    "                print(f\"[{name}] ep={ep:3d} loss={loss:.4f} acc={acc:.3f} | best={best_acc:.3f} @ep{best_ep}\")\n",
    "\n",
    "        # restore best for fair plotting & saving\n",
    "        if best_state is not None:\n",
    "            m.W1[:], m.b1[:], m.W2[:], m.b2[:] = best_state\n",
    "\n",
    "        print(f\"--- {name} best acc={best_acc:.3f} @ep{best_ep} ---\")\n",
    "\n",
    "        if best_acc > best_global[0]:\n",
    "            best_global = (best_acc, name, best_state)\n",
    "\n",
    "    # plots\n",
    "    ep_axis = np.arange(1, epochs + 1)\n",
    "    plt.figure(figsize=(16, 10))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for name, m in models:\n",
    "        plt.plot(ep_axis, m.loss_hist, label=name)\n",
    "    plt.title(\"Loss Karşılaştırma\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for name, m in models:\n",
    "        plt.plot(ep_axis, m.acc_hist, label=name)\n",
    "    plt.title(\"Test Accuracy Karşılaştırma\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for name, m in models:\n",
    "        if len(m.wdelta_hist) == len(ep_axis):\n",
    "            plt.plot(ep_axis, m.wdelta_hist, label=name)\n",
    "    plt.title(\"Yakınsama: ||W_t-W_{t-1}||\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Norm\"); plt.legend()\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for name, m in models:\n",
    "        if len(m.step_hist) == len(ep_axis):\n",
    "            plt.plot(ep_axis, m.step_hist, label=name)\n",
    "    plt.title(\"Adım Büyüklüğü\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Avg ||update||\"); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    best_acc, best_name, best_state = best_global\n",
    "    print(\"Best model:\", best_name, \"best acc:\", best_acc)\n",
    "\n",
    "    # confusion matrix for best model\n",
    "    best_model = None\n",
    "    for nm, m in models:\n",
    "        if nm == best_name:\n",
    "            best_model = m\n",
    "            break\n",
    "    pred = best_model.predict(X_test_s)\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    np.savez_compressed(\n",
    "        save_model,\n",
    "        model_name=best_name,\n",
    "        W1=best_model.W1, b1=best_model.b1, W2=best_model.W2, b2=best_model.b2,\n",
    "        mean=scaler.mean_.astype(np.float32),\n",
    "        scale=scaler.scale_.astype(np.float32),\n",
    "        classes=classes,\n",
    "        mode=\"face_only\",\n",
    "        hidden=hidden\n",
    "    )\n",
    "    print(\"Saved model:\", save_model)\n",
    "\n"
   ],
   "id": "541f14b5e8138a55",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "9) Model load + webcam",
   "id": "2cb84a7b3eac1b59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T13:53:07.781254Z",
     "start_time": "2025-12-17T13:53:07.765880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model_npz(model_npz):\n",
    "    d = np.load(model_npz, allow_pickle=True)\n",
    "    classes = d[\"classes\"].astype(str)\n",
    "    mode = str(d[\"mode\"])\n",
    "    W1 = d[\"W1\"].astype(np.float32); b1 = d[\"b1\"].astype(np.float32)\n",
    "    W2 = d[\"W2\"].astype(np.float32); b2 = d[\"b2\"].astype(np.float32)\n",
    "    mean = d[\"mean\"].astype(np.float32)\n",
    "    scale = d[\"scale\"].astype(np.float32)\n",
    "    return classes, mode, (W1, b1, W2, b2), mean, scale\n",
    "\n",
    "def forward_np(X, params):\n",
    "    W1, b1, W2, b2 = params\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    return softmax(z2)\n",
    "\n",
    "def webcam_demo(model_npz, cam_index=0, resize_to=384):\n",
    "    classes, mode, params, mean, scale = load_model_npz(model_npz)\n",
    "    dim = expected_feature_dim(mode)\n",
    "\n",
    "    cap = cv2.VideoCapture(cam_index)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Webcam açılamadı.\")\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    if mode == \"face_only\":\n",
    "        with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.2) as face_det, \\\n",
    "             mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1,\n",
    "                                   refine_landmarks=False,\n",
    "                                   min_detection_confidence=0.2,\n",
    "                                   min_tracking_confidence=0.2) as face_mesh:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                feat = facemesh_features_from_bgr(face_det, face_mesh, frame, resize_to=resize_to)\n",
    "                if feat is not None and feat.shape[0] == dim:\n",
    "                    x = feat.reshape(1, -1).astype(np.float32)\n",
    "                    x = (x - mean) / (scale + 1e-8)\n",
    "                    p = forward_np(x, params)[0]\n",
    "                    cls_idx = int(np.argmax(p))\n",
    "                    cls_name = classes[cls_idx]\n",
    "                    conf = float(np.max(p))\n",
    "                    preds.append(cls_name)\n",
    "                    stable = majority_vote(np.array(preds, dtype=object), k=15)\n",
    "                    text = f\"{cls_name} ({conf:.2f}) | stable: {stable}\"\n",
    "                else:\n",
    "                    text = \"No face landmarks\"\n",
    "\n",
    "                cv2.rectangle(frame, (0, 0), (620, 60), (0, 0, 0), -1)\n",
    "                cv2.putText(frame, text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
    "                cv2.imshow(\"KarcıFANN Live\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "    else:\n",
    "        with mp_holistic.Holistic(static_image_mode=False, model_complexity=1,\n",
    "                                  refine_face_landmarks=True,\n",
    "                                  min_detection_confidence=0.5,\n",
    "                                  min_tracking_confidence=0.5) as holistic:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                if resize_to is not None:\n",
    "                    frame = cv2.resize(frame, (resize_to, resize_to), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                rgb.flags.writeable = False\n",
    "                res = holistic.process(rgb)\n",
    "\n",
    "                feat = extract_features_from_results(res, mode=mode)\n",
    "                if feat.shape[0] == dim:\n",
    "                    x = feat.reshape(1, -1).astype(np.float32)\n",
    "                    x = (x - mean) / (scale + 1e-8)\n",
    "                    p = forward_np(x, params)[0]\n",
    "                    cls_idx = int(np.argmax(p))\n",
    "                    cls_name = classes[cls_idx]\n",
    "                    conf = float(np.max(p))\n",
    "                    preds.append(cls_name)\n",
    "                    stable = majority_vote(np.array(preds, dtype=object), k=15)\n",
    "                    text = f\"{cls_name} ({conf:.2f}) | stable: {stable}\"\n",
    "                else:\n",
    "                    text = \"No landmarks\"\n",
    "\n",
    "                cv2.rectangle(frame, (0, 0), (620, 60), (0, 0, 0), -1)\n",
    "                cv2.putText(frame, text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
    "                cv2.imshow(\"KarcıFANN Live\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ],
   "id": "f95f79f8c0afb45a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "10) Çalıştırma hücreleri",
   "id": "dd96b142d488bc78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "10.a Extract",
   "id": "74660cf91681e464"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:41:55.757309Z",
     "start_time": "2025-12-17T06:24:30.655308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET_ROOT = r\".\\datasetKaggleFER2013\"\n",
    "# 1) Extract (align edilmiş)\n",
    "extract_fer2013_folder(\n",
    "    root=DATASET_ROOT,\n",
    "    out=\"fer_mp_align.npz\",\n",
    "    resize_to=384\n",
    ")\n"
   ],
   "id": "3f1ff20f9f3383fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "[train] angry: 3995 görüntü\n",
      "[train] disgust: 436 görüntü\n",
      "[train] fear: 4097 görüntü\n",
      "[train] happy: 7215 görüntü\n",
      "[train] neutral: 4965 görüntü\n",
      "[train] sad: 4830 görüntü\n",
      "[train] surprise: 3171 görüntü\n",
      "[test] angry: 958 görüntü\n",
      "[test] disgust: 111 görüntü\n",
      "[test] fear: 1024 görüntü\n",
      "[test] happy: 1774 görüntü\n",
      "[test] neutral: 1233 görüntü\n",
      "[test] sad: 1247 görüntü\n",
      "[test] surprise: 831 görüntü\n",
      "\n",
      "Saved: fer_mp_align.npz\n",
      "Train: (26386, 1872) Test: (6583, 1872)\n",
      "Used: {'train': 26386, 'test': 6583} Skipped: {'train': 2323, 'test': 595}\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "10.b Train",
   "id": "a3847a7cdb9cc7fa"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-17T06:41:55.795342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2) Train+Compare\n",
    "train_compare(\n",
    "    data_npz=\"fer_mp_align.npz\",\n",
    "    epochs=150,\n",
    "    hidden=512,\n",
    "    batch=128,\n",
    "    save_model=\"fer_model_best.npz\"\n",
    ")"
   ],
   "id": "7394ffab4a2f86a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (26386, 1872) Test: (6583, 1872) Classes: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n",
      "Class weights: [0.491 4.494 0.456 0.244 0.357 0.395 0.562]\n",
      "[SGD] ep= 10 loss=1.3241 acc=0.347 | best=0.379 @ep8\n",
      "[SGD] ep= 20 loss=0.8475 acc=0.358 | best=0.433 @ep19\n",
      "[SGD] ep= 30 loss=0.6619 acc=0.361 | best=0.464 @ep29\n",
      "[SGD] ep= 40 loss=0.6131 acc=0.365 | best=0.464 @ep29\n",
      "[SGD] ep= 50 loss=0.5829 acc=0.438 | best=0.480 @ep45\n",
      "[SGD] ep= 60 loss=0.5766 acc=0.402 | best=0.480 @ep45\n",
      "[SGD] ep= 70 loss=0.5643 acc=0.450 | best=0.480 @ep45\n",
      "[SGD] ep= 80 loss=0.5565 acc=0.416 | best=0.487 @ep79\n",
      "[SGD] ep= 90 loss=0.5532 acc=0.413 | best=0.487 @ep79\n",
      "[SGD] ep=100 loss=0.5540 acc=0.445 | best=0.487 @ep79\n",
      "[SGD] ep=110 loss=0.5370 acc=0.408 | best=0.487 @ep79\n",
      "[SGD] ep=120 loss=0.5376 acc=0.477 | best=0.487 @ep79\n",
      "[SGD] ep=130 loss=0.5323 acc=0.487 | best=0.487 @ep130\n",
      "[SGD] ep=140 loss=0.5313 acc=0.432 | best=0.502 @ep133\n",
      "[SGD] ep=150 loss=0.5309 acc=0.439 | best=0.502 @ep133\n",
      "--- SGD best acc=0.502 @ep133 ---\n",
      "[Adam] ep= 10 loss=0.7947 acc=0.405 | best=0.420 @ep9\n",
      "[Adam] ep= 20 loss=0.6813 acc=0.444 | best=0.467 @ep18\n",
      "[Adam] ep= 30 loss=0.6827 acc=0.449 | best=0.467 @ep18\n",
      "[Adam] ep= 40 loss=0.6804 acc=0.454 | best=0.467 @ep18\n",
      "[Adam] ep= 50 loss=0.6657 acc=0.396 | best=0.467 @ep18\n",
      "[Adam] ep= 60 loss=0.6587 acc=0.439 | best=0.484 @ep56\n",
      "[Adam] ep= 70 loss=0.6571 acc=0.440 | best=0.484 @ep56\n",
      "[Adam] ep= 80 loss=0.6548 acc=0.453 | best=0.484 @ep56\n",
      "[Adam] ep= 90 loss=0.6501 acc=0.439 | best=0.484 @ep56\n",
      "[Adam] ep=100 loss=0.6516 acc=0.433 | best=0.484 @ep56\n",
      "[Adam] ep=110 loss=0.6486 acc=0.449 | best=0.484 @ep56\n",
      "[Adam] ep=120 loss=0.6513 acc=0.428 | best=0.484 @ep56\n",
      "[Adam] ep=130 loss=0.6508 acc=0.461 | best=0.484 @ep56\n",
      "[Adam] ep=140 loss=0.6465 acc=0.453 | best=0.484 @ep56\n",
      "[Adam] ep=150 loss=0.6460 acc=0.443 | best=0.484 @ep56\n",
      "--- Adam best acc=0.484 @ep56 ---\n",
      "[KarcıFANN] ep= 10 loss=1.5045 acc=0.269 | best=0.333 @ep3\n",
      "[KarcıFANN] ep= 20 loss=1.1608 acc=0.127 | best=0.333 @ep3\n",
      "[KarcıFANN] ep= 30 loss=2.7924 acc=0.268 | best=0.362 @ep26\n",
      "[KarcıFANN] ep= 40 loss=2.1793 acc=0.288 | best=0.362 @ep26\n",
      "[KarcıFANN] ep= 50 loss=1.6370 acc=0.191 | best=0.362 @ep26\n",
      "[KarcıFANN] ep= 60 loss=0.9755 acc=0.260 | best=0.362 @ep26\n",
      "[KarcıFANN] ep= 70 loss=0.9070 acc=0.317 | best=0.362 @ep26\n",
      "[KarcıFANN] ep= 80 loss=0.9041 acc=0.295 | best=0.362 @ep26\n",
      "[KarcıFANN] ep= 90 loss=0.8336 acc=0.243 | best=0.362 @ep26\n",
      "[KarcıFANN] ep=100 loss=0.8679 acc=0.296 | best=0.362 @ep26\n",
      "[KarcıFANN] ep=110 loss=0.8286 acc=0.178 | best=0.362 @ep26\n",
      "[KarcıFANN] ep=120 loss=0.8371 acc=0.293 | best=0.362 @ep26\n",
      "[KarcıFANN] ep=130 loss=0.8283 acc=0.315 | best=0.362 @ep26\n",
      "[KarcıFANN] ep=140 loss=0.8445 acc=0.201 | best=0.362 @ep26\n",
      "[KarcıFANN] ep=150 loss=0.8256 acc=0.163 | best=0.362 @ep26\n",
      "--- KarcıFANN best acc=0.362 @ep26 ---\n",
      "[Momentum-Karcı] ep= 10 loss=6.8057 acc=0.322 | best=0.357 @ep2\n",
      "[Momentum-Karcı] ep= 20 loss=6.8281 acc=0.273 | best=0.357 @ep2\n",
      "[Momentum-Karcı] ep= 30 loss=1.3493 acc=0.157 | best=0.357 @ep2\n",
      "[Momentum-Karcı] ep= 40 loss=0.8716 acc=0.259 | best=0.357 @ep2\n",
      "[Momentum-Karcı] ep= 50 loss=0.8709 acc=0.014 | best=0.357 @ep2\n",
      "[Momentum-Karcı] ep= 60 loss=0.8721 acc=0.124 | best=0.357 @ep2\n",
      "[Momentum-Karcı] ep= 70 loss=0.8717 acc=0.141 | best=0.357 @ep2\n",
      "[Momentum-Karcı] ep= 80 loss=0.8725 acc=0.165 | best=0.357 @ep2\n",
      "[Momentum-Karcı] ep= 90 loss=0.8718 acc=0.165 | best=0.357 @ep2\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "10.c Webcam",
   "id": "ed6addc4f6eebb2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "webcam_demo(\"fer_model_best.npz\", cam_index=0, resize_to=384)",
   "id": "e3f06cf82d85085a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.Entegre edilmiş versiyonu",
   "id": "7882ce9e7a8e61d2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
