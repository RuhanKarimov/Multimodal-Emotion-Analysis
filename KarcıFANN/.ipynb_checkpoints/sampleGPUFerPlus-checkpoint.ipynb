{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T03:42:05.242755Z",
     "start_time": "2025-12-22T03:42:05.226798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, glob, sys\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"python:\", sys.executable)\n",
    "\n",
    "suspects = []\n",
    "for pat in [\"torch.py\", \"torch*.py\", \"types.py\", \"typing_extensions.py\", \"numpy.py\", \"cv2.py\"]:\n",
    "    suspects += glob.glob(os.path.join(os.getcwd(), pat))\n",
    "\n",
    "# paket klasörü gibi çakışmalar\n",
    "for d in [\"torch\", \"types\", \"typing_extensions\", \"numpy\", \"cv2\"]:\n",
    "    if os.path.isdir(os.path.join(os.getcwd(), d)):\n",
    "        suspects.append(os.path.join(os.getcwd(), d) + os.sep)\n",
    "\n",
    "print(\"SUSPECTS in cwd:\")\n",
    "for s in suspects:\n",
    "    print(\" -\", s)\n"
   ],
   "id": "84889c0cabdeea7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: C:\\Users\\ruhan\\PycharmProjects\\KarcıFANN\n",
      "python: C:\\Users\\ruhan\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n",
      "SUSPECTS in cwd:\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "7e49cf272aa2c8d1",
   "metadata": {},
   "source": "1) Config + Seed +  Device"
  },
  {
   "cell_type": "code",
   "id": "a98f048bc13f9b6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:52:17.369603Z",
     "start_time": "2025-12-22T02:52:17.348943Z"
    }
   },
   "source": [
    "# CELL 0: FERPlus label CSV indir\n",
    "import os, urllib.request\n",
    "\n",
    "os.makedirs(\"./ferplus\", exist_ok=True)\n",
    "\n",
    "FERPLUS_CSV_URL = \"https://raw.githubusercontent.com/microsoft/FERPlus/master/fer2013new.csv\"\n",
    "FERPLUS_CSV_PATH = \"./ferplus/.csv\"\n",
    "\n",
    "if not os.path.isfile(FERPLUS_CSV_PATH):\n",
    "    urllib.request.urlretrieve(FERPLUS_CSV_URL, FERPLUS_CSV_PATH)\n",
    "\n",
    "print(\"OK:\", FERPLUS_CSV_PATH)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: ./ferplus/fer2013new.csv\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2) Dataset + Transforms + Loaders + Class weights",
   "id": "c967a02628d5322f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:52:17.413158Z",
     "start_time": "2025-12-22T02:52:17.376829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CELL 1: FER2013 csv oku\n",
    "import pandas as pd\n",
    "\n",
    "FER2013_CSV_PATH = r\"./ferplus/fer2013new.csv\"   # sende neredeyse onu yaz\n",
    "df = pd.read_csv(FER2013_CSV_PATH)\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df[\"Usage\"].value_counts())\n"
   ],
   "id": "575efb0b913a937f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 12)\n",
      "Index(['Usage', 'Image name', 'neutral', 'happiness', 'surprise', 'sadness',\n",
      "       'anger', 'disgust', 'fear', 'contempt', 'unknown', 'NF'],\n",
      "      dtype='object')\n",
      "Usage\n",
      "Training       28709\n",
      "PublicTest      3589\n",
      "PrivateTest     3589\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3) Model(ResNet34 öneriyorum) + Freeze/Unfreeze",
   "id": "f9f2169f2adb4a44"
  },
  {
   "cell_type": "code",
   "id": "57eb289c8b20e90d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T03:31:04.338483Z",
     "start_time": "2025-12-22T03:31:04.163780Z"
    }
   },
   "source": [
    "# CELL 2 (FIX): FERPlus label csv oku + 7 sınıfa indir (robust)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) CSV'yi header ile oku\n",
    "ferp = pd.read_csv(FERPLUS_CSV_PATH)   # header=0 default\n",
    "print(\"ferplus shape:\", ferp.shape)\n",
    "print(\"columns:\", list(ferp.columns))\n",
    "print(ferp.head(2))\n",
    "\n",
    "# 2) Beklenen kolonlar\n",
    "# FERPlus standard sırası:\n",
    "# neutral, happiness, surprise, sadness, anger, disgust, fear, contempt, unknown, NF\n",
    "base_cols = [\"neutral\",\"happiness\",\"surprise\",\"sadness\",\"anger\",\"disgust\",\"fear\",\"contempt\"]\n",
    "extra_cols = [\"unknown\",\"NF\"]\n",
    "\n",
    "missing = [c for c in base_cols if c not in ferp.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV içinde bu kolonlar yok: {missing}. Kolonlar: {list(ferp.columns)}\")\n",
    "\n",
    "# 3) Sayısala çevir (emin olmak için)\n",
    "counts_8 = ferp[base_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).to_numpy(dtype=np.float32)\n",
    "\n",
    "contempt = counts_8[:, 7].copy()  # contempt kolonu\n",
    "# unknown/NF varsa al (yoksa 0)\n",
    "unknown = ferp[extra_cols[0]].apply(pd.to_numeric, errors=\"coerce\").fillna(0).to_numpy(np.float32) if extra_cols[0] in ferp.columns else 0.0\n",
    "nf      = ferp[extra_cols[1]].apply(pd.to_numeric, errors=\"coerce\").fillna(0).to_numpy(np.float32) if extra_cols[1] in ferp.columns else 0.0\n",
    "\n",
    "# 4) 7 sınıfa map (senin class sıran)\n",
    "# 8 indeks: 0 neutral,1 happy,2 surprise,3 sad,4 angry,5 disgust,6 fear,7 contempt\n",
    "idx7 = [4,5,6,1,0,3,2]  # angry,disgust,fear,happy,neutral,sad,surprise\n",
    "counts_7 = counts_8[:, idx7]\n",
    "\n",
    "sum7 = counts_7.sum(axis=1)\n",
    "\n",
    "# 5) Filtre (minimum güvenlik)\n",
    "keep = (sum7 > 0)\n",
    "\n",
    "# İstersen daha agresif filtre: unknown/NF yüksekse at\n",
    "if isinstance(unknown, np.ndarray):\n",
    "    keep = keep & (unknown <= 0)\n",
    "if isinstance(nf, np.ndarray):\n",
    "    keep = keep & (nf <= 0)\n",
    "\n",
    "# İstersen contempt baskınsa at (7 sınıf dışı)\n",
    "# keep = keep & (contempt < sum7)\n",
    "\n",
    "counts_7 = counts_7[keep]\n",
    "\n",
    "# 6) FER2013 df ile satır hizası\n",
    "# df: FER2013 dataframe (pixels, emotion vs) sende daha önce yüklenmiş olmalı\n",
    "if \"df\" not in globals():\n",
    "    raise NameError(\"df değişkeni bulunamadı. FER2013 csv'yi df olarak önce yüklemelisin.\")\n",
    "\n",
    "if len(df) != len(ferp):\n",
    "    print(\"UYARI: df ile ferp satır sayısı eşit değil!\")\n",
    "    print(\"len(df)=\", len(df), \"len(ferp)=\", len(ferp))\n",
    "\n",
    "df2 = df.loc[keep].reset_index(drop=True)\n",
    "\n",
    "# 7) soft/hard label üret\n",
    "soft = counts_7 / (counts_7.sum(axis=1, keepdims=True) + 1e-8)\n",
    "hard = soft.argmax(axis=1).astype(np.int64)\n",
    "\n",
    "classes = [\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprise\"]\n",
    "\n",
    "print(\"after filter:\", df2.shape, soft.shape, hard.shape)\n",
    "print(\"class dist:\", np.bincount(hard, minlength=7))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ferplus shape: (35887, 12)\n",
      "columns: ['Usage', 'Image name', 'neutral', 'happiness', 'surprise', 'sadness', 'anger', 'disgust', 'fear', 'contempt', 'unknown', 'NF']\n",
      "      Usage      Image name  neutral  happiness  surprise  sadness  anger  \\\n",
      "0  Training  fer0000000.png        4          0         0        1      3   \n",
      "1  Training  fer0000001.png        6          0         1        1      0   \n",
      "\n",
      "   disgust  fear  contempt  unknown  NF  \n",
      "0        2     0         0        0   0  \n",
      "1        0     0         0        2   0  \n",
      "after filter: (23736, 12) (23736, 7) (23736,)\n",
      "class dist: [1905  157  614 8374 7240 2400 3046]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4) Karcı Optimizer(CNN için) + Momentum-Karcı",
   "id": "8bc4c10c7fb8a5bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T03:31:25.864495Z",
     "start_time": "2025-12-22T03:31:25.499422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CELL 3: Dataset (FIX)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class FERPlusCSVDataset(Dataset):\n",
    "    def __init__(self, df_fer, soft_targets, hard_targets,\n",
    "                 img_size=224, use_soft=True, to_3ch=True):\n",
    "        self.df = df_fer.reset_index(drop=True)\n",
    "        self.soft = np.asarray(soft_targets, dtype=np.float32)\n",
    "        self.hard = np.asarray(hard_targets, dtype=np.int64)\n",
    "        self.use_soft = bool(use_soft)\n",
    "        self.to_3ch = bool(to_3ch)\n",
    "        self.img_size = int(img_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        pix = self.df.loc[i, \"pixels\"]\n",
    "        arr = np.fromstring(pix, sep=\" \", dtype=np.uint8).reshape(48, 48)\n",
    "        im = Image.fromarray(arr, mode=\"L\")  # grayscale\n",
    "        im = im.resize((self.img_size, self.img_size), resample=Image.BILINEAR)\n",
    "\n",
    "        x = torch.from_numpy(np.array(im, dtype=np.float32) / 255.0).unsqueeze(0)  # [1,H,W]\n",
    "        x = (x - 0.5) / 0.5\n",
    "\n",
    "        if self.to_3ch:\n",
    "            x = x.repeat(3, 1, 1)  # [3,H,W]\n",
    "\n",
    "        if self.use_soft:\n",
    "            y = torch.from_numpy(self.soft[i]).float()   # [7]\n",
    "        else:\n",
    "            y = torch.tensor(int(self.hard[i]), dtype=torch.long)  # scalar\n",
    "\n",
    "        return x, y\n"
   ],
   "id": "cbe601137691b61d",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'types' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# CELL 3: Dataset (FIX)\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mPIL\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Image\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py:2126\u001B[0m\n\u001B[0;32m   2119\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compile\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _disable_dynamo  \u001B[38;5;66;03m# usort: skip\u001B[39;00m\n\u001B[0;32m   2121\u001B[0m \u001B[38;5;66;03m################################################################################\u001B[39;00m\n\u001B[0;32m   2122\u001B[0m \u001B[38;5;66;03m# Import interface functions defined in Python\u001B[39;00m\n\u001B[0;32m   2123\u001B[0m \u001B[38;5;66;03m################################################################################\u001B[39;00m\n\u001B[0;32m   2124\u001B[0m \n\u001B[0;32m   2125\u001B[0m \u001B[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001B[39;00m\n\u001B[1;32m-> 2126\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _VF \u001B[38;5;28;01mas\u001B[39;00m _VF, functional \u001B[38;5;28;01mas\u001B[39;00m functional  \u001B[38;5;66;03m# usort: skip\u001B[39;00m\n\u001B[0;32m   2127\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# usort: skip # noqa: F403\u001B[39;00m\n\u001B[0;32m   2129\u001B[0m \u001B[38;5;66;03m################################################################################\u001B[39;00m\n\u001B[0;32m   2130\u001B[0m \u001B[38;5;66;03m# Remove unnecessary members\u001B[39;00m\n\u001B[0;32m   2131\u001B[0m \u001B[38;5;66;03m################################################################################\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtyping\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Any, Optional, TYPE_CHECKING, Union\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mF\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _VF, Tensor\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_C\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m _add_docstr\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\__init__.py:8\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# mypy: allow-untyped-defs\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparameter\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# usort: skip\u001B[39;00m\n\u001B[0;32m      3\u001B[0m     Buffer \u001B[38;5;28;01mas\u001B[39;00m Buffer,\n\u001B[0;32m      4\u001B[0m     Parameter \u001B[38;5;28;01mas\u001B[39;00m Parameter,\n\u001B[0;32m      5\u001B[0m     UninitializedBuffer \u001B[38;5;28;01mas\u001B[39;00m UninitializedBuffer,\n\u001B[0;32m      6\u001B[0m     UninitializedParameter \u001B[38;5;28;01mas\u001B[39;00m UninitializedParameter,\n\u001B[0;32m      7\u001B[0m )\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodules\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m  \u001B[38;5;66;03m# usort: skip # noqa: F403\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     10\u001B[0m     attention \u001B[38;5;28;01mas\u001B[39;00m attention,\n\u001B[0;32m     11\u001B[0m     functional \u001B[38;5;28;01mas\u001B[39;00m functional,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     16\u001B[0m     utils \u001B[38;5;28;01mas\u001B[39;00m utils,\n\u001B[0;32m     17\u001B[0m )\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparallel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataParallel \u001B[38;5;28;01mas\u001B[39;00m DataParallel\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodule\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Module  \u001B[38;5;66;03m# usort: skip\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinear\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Bilinear, Identity, LazyLinear, Linear  \u001B[38;5;66;03m# usort: skip\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivation\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      4\u001B[0m     CELU,\n\u001B[0;32m      5\u001B[0m     ELU,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     32\u001B[0m     Threshold,\n\u001B[0;32m     33\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_prims_common\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DeviceLikeType\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparameter\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Buffer, Parameter\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_python_dispatch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_traceable_wrapper_subclass\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhooks\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BackwardHook, RemovableHandle\n\u001B[0;32m     21\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregister_module_forward_pre_hook\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mregister_module_forward_hook\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     31\u001B[0m ]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_python_dispatch.py:296\u001B[0m\n\u001B[0;32m    292\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    295\u001B[0m \u001B[38;5;66;03m# Subtypes which have __tensor_flatten__ and __tensor_unflatten__.\u001B[39;00m\n\u001B[1;32m--> 296\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mTensorWithFlatten\u001B[39;00m(Protocol):\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__tensor_flatten__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[Sequence[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;28mobject\u001B[39m]:\n\u001B[0;32m    298\u001B[0m         \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_python_dispatch.py:335\u001B[0m, in \u001B[0;36mTensorWithFlatten\u001B[1;34m()\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdim\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;129m@overload\u001B[39m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mto\u001B[39m(\n\u001B[0;32m    334\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m--> 335\u001B[0m         dtype: \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtypes\u001B[49m\u001B[38;5;241m.\u001B[39m_dtype,\n\u001B[0;32m    336\u001B[0m         non_blocking: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    337\u001B[0m         copy: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    338\u001B[0m         \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m    339\u001B[0m         memory_format: Optional[torch\u001B[38;5;241m.\u001B[39mmemory_format] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    340\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m    341\u001B[0m     \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;129m@overload\u001B[39m\n\u001B[0;32m    344\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mto\u001B[39m(\n\u001B[0;32m    345\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    351\u001B[0m         memory_format: Optional[torch\u001B[38;5;241m.\u001B[39mmemory_format] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    352\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n",
      "\u001B[1;31mAttributeError\u001B[0m: partially initialized module 'torch' has no attribute 'types' (most likely due to a circular import)"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5) Optimizer factory + Loss",
   "id": "a87872222d4f070e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:52:17.994678900Z",
     "start_time": "2025-12-21T18:03:02.200273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CELL 4: Split + DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH = 256\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_mask = (df2[\"Usage\"] == \"Training\")\n",
    "test_mask  = (df2[\"Usage\"] != \"Training\")   # PublicTest + PrivateTest\n",
    "\n",
    "ds_train = FERPlusCSVDataset(df2[train_mask], soft[train_mask.values], hard[train_mask.values], img_size=IMG_SIZE, use_soft=True)\n",
    "ds_test  = FERPlusCSVDataset(df2[test_mask],  soft[test_mask.values],  hard[test_mask.values],  img_size=IMG_SIZE, use_soft=False)  # eval hard\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=BATCH, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(ds_test,  batch_size=BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "len(ds_train), len(ds_test)\n"
   ],
   "id": "d032e95c1ceaba47",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6)Train/Eval(AMP doğru kullanımı + scheduler sırası düzeltildi)",
   "id": "9a6f11bfab058147"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:52:17.994678900Z",
     "start_time": "2025-12-21T18:03:02.235526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CELL 5: Soft Cross Entropy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def soft_cross_entropy(logits, soft_targets):\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    return -(soft_targets * logp).sum(dim=1).mean()\n"
   ],
   "id": "baff6d72bd344fc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7)Tek optimizer çalıştırma(her biri ayrı kayıt) + OneCycleLR fix(cycle_momentum)",
   "id": "c3f760604a514987"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T02:52:17.994678900Z",
     "start_time": "2025-12-22T01:43:32.845278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CELL 6: Train/Eval (soft uyumlu)\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler_amp = torch.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)  # hard: [B], soft kullanmıyoruz testte\n",
    "\n",
    "        with torch.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):\n",
    "            logits = model(xb)\n",
    "\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        ys.append(yb.detach().cpu().numpy())\n",
    "        ps.append(pred.detach().cpu().numpy())\n",
    "\n",
    "    y = np.concatenate(ys)\n",
    "    p = np.concatenate(ps)\n",
    "    return float(accuracy_score(y, p)), y, p\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer,\n",
    "                    scheduler=None, karci=False, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.autocast(device_type=device.type, enabled=(device.type == \"cuda\")):\n",
    "            logits = model(xb)\n",
    "            # yb soft ise [B,C], hard ise [B]\n",
    "            if yb.ndim == 2:\n",
    "                loss = soft_cross_entropy(logits, yb)\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "        scaler_amp.scale(loss).backward()\n",
    "        scaler_amp.unscale_(optimizer)\n",
    "        if grad_clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        if karci:\n",
    "            optimizer.step(float(loss.detach().item()))\n",
    "        else:\n",
    "            scaler_amp.step(optimizer)\n",
    "\n",
    "        scaler_amp.update()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        total_loss += float(loss.detach().item()) * bs\n",
    "        n += bs\n",
    "\n",
    "    return total_loss / max(1, n)\n"
   ],
   "id": "e94efd6cc8276c8a",
   "outputs": [],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (CUDA cu118)",
   "language": "python",
   "name": "py310-cu118"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
